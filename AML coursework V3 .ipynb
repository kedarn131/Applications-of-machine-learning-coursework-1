{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7992ab8",
   "metadata": {},
   "source": [
    "# Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "366fffac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all the libraries. If any library is missing, then you can use \"pip install library name\". \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os \n",
    "from sklearn.utils import shuffle\n",
    "import nltk \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report,accuracy_score,mean_squared_error,mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5404d3d3",
   "metadata": {},
   "source": [
    "# Importing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb200146",
   "metadata": {},
   "outputs": [],
   "source": [
    "direct = []\n",
    "file = []\n",
    "title = []\n",
    "text = []\n",
    "label = []\n",
    "dpath = '/Users/kedarnandiwdekar/Desktop/datasets /' # saved directory path in a variable \n",
    "for dirname, _ , filenames in os.walk(dpath):\n",
    "    try:\n",
    "        filenames.remove('README.TXT') # checking if theres a read me file. If true, then deleting it.\n",
    "    except:\n",
    "        pass\n",
    "    for filename in filenames:\n",
    "        direct.append(dirname)\n",
    "        file.append(filename)\n",
    "        label.append(dirname.split('/')[-1])\n",
    "        fullpathfile = os.path.join(dirname,filename)\n",
    "        with open(fullpathfile, 'r', encoding=\"utf8\", errors='ignore') as infile: # opening the file from list of files in read mode \n",
    "            intext = ''\n",
    "            firstline = True\n",
    "            for line in infile:\n",
    "                if firstline:\n",
    "                    title.append(line.replace('\\n',''))\n",
    "                    firstline = False\n",
    "                else:\n",
    "                    intext = intext + ' ' + line.replace('\\n','')\n",
    "            text.append(intext)\n",
    "df = pd.DataFrame(list(zip(text, label)), \n",
    "               columns =['text', 'label'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522d014f",
   "metadata": {},
   "source": [
    "# Dividing the dataset into two subsets train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37d4912e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = shuffle(df) # Basically will shuffle the dataset to reduce the bias. \n",
    "df['category_id'] = df['label'].factorize()[0] # factorize will give numeric values to categories. \n",
    "train = df.iloc[:1780,:] # This is to divide the dataset into 80:20\n",
    "test = df.iloc[1781:2225,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda3c6b1",
   "metadata": {},
   "source": [
    "# Feature engineering  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c28fd1",
   "metadata": {},
   "source": [
    "# Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5618df28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h8/4m_c3pfj1cnd2kpvtxkflc600000gn/T/ipykernel_5813/1944099544.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train['text'] = train['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "223     Scottish rock band Franz Ferdinand, shot promi...\n",
       "1586    Thousands civil service jobs already cut moved...\n",
       "567     US economic growth accelerated third quarter, ...\n",
       "163     Oscar-nominated animation Shark Tale raked $80...\n",
       "119     French booksellers braced rush interest anothe...\n",
       "                              ...                        \n",
       "1128    Graeme Souness believes Walter Smith would per...\n",
       "1269    Hearts wrapped Scottish Cup quarter-final tie ...\n",
       "1785    Labour's choice white candidate one UK's multi...\n",
       "1693    Labour already broken pre-election promise imm...\n",
       "331     Preview performances Â£3m musical Billy Elliot ...\n",
       "Name: text, Length: 1780, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = set(stopwords.words('english')) \n",
    "stop = stopwords.words('english') # Basically, we will save all the stop words collection in a list\n",
    "train['text'] = train['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)])) # we will match words in ur text to the ones from the list\n",
    "train['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75758752",
   "metadata": {},
   "source": [
    "# Stemming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52f80481",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h8/4m_c3pfj1cnd2kpvtxkflc600000gn/T/ipykernel_5813/4294212689.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train['text'] = train['text'].apply(lambda x: ' '.join([ps.stem(word) for word in x.split()]))\n",
      "/var/folders/h8/4m_c3pfj1cnd2kpvtxkflc600000gn/T/ipykernel_5813/4294212689.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  train['text'] = train['text'].str.replace('[^\\w\\s]','')\n",
      "/var/folders/h8/4m_c3pfj1cnd2kpvtxkflc600000gn/T/ipykernel_5813/4294212689.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train['text'] = train['text'].str.replace('[^\\w\\s]','')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "223     scottish rock band franz ferdinand shot promin...\n",
       "1586    thousand civil servic job alreadi cut move lon...\n",
       "567     us econom growth acceler third quarter help st...\n",
       "163     oscarnomin anim shark tale rake 80m 424m first...\n",
       "119     french booksel brace rush interest anoth book ...\n",
       "                              ...                        \n",
       "1128    graem souness believ walter smith would perfec...\n",
       "1269    heart wrap scottish cup quarterfin tie livings...\n",
       "1785    labour choic white candid one uk multiraci sea...\n",
       "1693    labour alreadi broken preelect promis immigr i...\n",
       "331     preview perform 3m music billi elliot delay gi...\n",
       "Name: text, Length: 1780, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "train['text'] = train['text'].apply(lambda x: ' '.join([ps.stem(word) for word in x.split()]))\n",
    "train['text'] = train['text'].str.replace('[^\\w\\s]','')\n",
    "train['text']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db506750",
   "metadata": {},
   "source": [
    "# Lemmetization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8b8dee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "223     scottish rock band franz ferdinand shot promin...\n",
       "1586    thousand civil servic job alreadi cut move lon...\n",
       "567     us econom growth acceler third quarter help st...\n",
       "163     oscarnomin anim shark tale rake 80m 424m first...\n",
       "119     french booksel brace rush interest anoth book ...\n",
       "                              ...                        \n",
       "1128    graem souness believ walter smith would perfec...\n",
       "1269    heart wrap scottish cup quarterfin tie livings...\n",
       "1785    labour choic white candid one uk multiraci sea...\n",
       "1693    labour alreadi broken preelect promis immigr i...\n",
       "331     preview perform 3m music billi elliot delay gi...\n",
       "Name: text, Length: 1780, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "wordnet_lemm = WordNetLemmatizer()\n",
    "lemm_words = np.vectorize(wordnet_lemm.lemmatize)\n",
    "lemm_text = ' '.join(lemm_words(train['text']))\n",
    "train['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18f455e",
   "metadata": {},
   "source": [
    "# Splitting words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2eadd523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27724,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count = pd.Series(' '.join(train['text']).split()).value_counts()\n",
    "word_count.sample(10)\n",
    "word_count.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c325579",
   "metadata": {},
   "source": [
    "#  TFIDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "891225c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english', lowercase = True)\n",
    "# With this method we will split our text into one and two words combination. \n",
    "# Depending upon their usage and importance they will be assigned a value.  \n",
    "news = tfidf.fit_transform(train['text']).toarray()\n",
    "ids = train['category_id']\n",
    "print(news)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7856308",
   "metadata": {},
   "source": [
    "# Feature selection using chi-square test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1986ea16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0329645, 0.       , 0.       , ..., 0.       , 0.       ,\n",
       "        0.       ],\n",
       "       [0.       , 0.       , 0.       , ..., 0.       , 0.       ,\n",
       "        0.       ],\n",
       "       [0.       , 0.       , 0.       , ..., 0.       , 0.       ,\n",
       "        0.       ],\n",
       "       ...,\n",
       "       [0.       , 0.       , 0.       , ..., 0.       , 0.       ,\n",
       "        0.       ],\n",
       "       [0.       , 0.       , 0.       , ..., 0.       , 0.       ,\n",
       "        0.       ],\n",
       "       [0.       , 0.       , 0.       , ..., 0.       , 0.       ,\n",
       "        0.       ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest \n",
    "from sklearn.feature_selection import chi2\n",
    "kbest = SelectKBest(score_func = chi2, k = 500 ) # Here using the chi-square test, we will determine which top 500 features will be retained. \n",
    "best_news = kbest.fit_transform(news, ids)\n",
    "best_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0656edc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "223     scottish rock band franz ferdinand shot promin...\n",
       "1586    thousand civil servic job alreadi cut move lon...\n",
       "567     us econom growth acceler third quarter help st...\n",
       "163     oscarnomin anim shark tale rake 80m 424m first...\n",
       "119     french booksel brace rush interest anoth book ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = train['text']\n",
    "category = train['label']\n",
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c2dd0ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "223     entertainment\n",
       "1586         politics\n",
       "567          business\n",
       "163     entertainment\n",
       "119     entertainment\n",
       "            ...      \n",
       "1128            sport\n",
       "1269            sport\n",
       "1785         politics\n",
       "1693         politics\n",
       "331     entertainment\n",
       "Name: label, Length: 1780, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8791b3",
   "metadata": {},
   "source": [
    "# Splitting into train and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1e98a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(best_news,ids, test_size = 0.3, random_state = 60,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e485e00d",
   "metadata": {},
   "source": [
    "# Model training and cross validation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09db51a4",
   "metadata": {},
   "source": [
    "First we will use MultinomialNB model and train the model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f499f4b",
   "metadata": {},
   "source": [
    "# MultinomialNB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20c8bfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we will first test MultinomialNB model without cross validation.\n",
    "from sklearn.model_selection import StratifiedKFold  \n",
    "from sklearn.metrics import confusion_matrix\n",
    "multi = MultinomialNB()\n",
    "multi.fit(X_train, Y_train) # fit method allows us to fit our data into the model \n",
    "Y_pred = multi.predict(X_test) # using predict() we will predict the dependent values for the corresponding independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1068793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.90      0.94        99\n",
      "           1       0.98      0.97      0.98       101\n",
      "           2       0.93      0.96      0.94       116\n",
      "           3       0.97      0.99      0.98       128\n",
      "           4       0.92      0.96      0.94        90\n",
      "\n",
      "    accuracy                           0.96       534\n",
      "   macro avg       0.96      0.95      0.96       534\n",
      "weighted avg       0.96      0.96      0.96       534\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(Y_test,Y_pred)) # This is used to give the classification report for the predicted values against the original values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b20ba2",
   "metadata": {},
   "source": [
    "Now we cross validate using k-fold cross validation technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ef29f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kedarnandiwdekar/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_split.py:666: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.96      , 0.944     , 0.92      , 0.944     , 0.936     ,\n",
       "       0.96      , 0.97580645, 0.9516129 , 0.95967742, 0.92741935])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "multi.fit(X_train, Y_train)\n",
    "scores = cross_val_score(multi, X_train, Y_train, cv=10)\n",
    "scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f47f829d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9478516129032257, 0.0161057651533809)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.mean(), scores.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde356c5",
   "metadata": {},
   "source": [
    "# Decision Tree "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c82c8e",
   "metadata": {},
   "source": [
    "Now we will use cross validation with decision tree model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a09b9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.78      0.77        99\n",
      "           1       0.82      0.88      0.85       101\n",
      "           2       0.86      0.76      0.81       116\n",
      "           3       0.91      0.90      0.90       128\n",
      "           4       0.81      0.87      0.84        90\n",
      "\n",
      "    accuracy                           0.84       534\n",
      "   macro avg       0.83      0.84      0.83       534\n",
      "weighted avg       0.84      0.84      0.84       534\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kedarnandiwdekar/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_split.py:666: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now the scores after validation are:  [0.84       0.864      0.88       0.832      0.848      0.872\n",
      " 0.82258065 0.85483871 0.81451613 0.75806452]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8385999999999999, 0.03345873940326765)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier  \n",
    "dtree = DecisionTreeClassifier()\n",
    "dtree.fit(X_train, Y_train)\n",
    "Y_pred = dtree.predict(X_test)\n",
    "print(classification_report(Y_test, Y_pred))\n",
    "dtree.fit(X_train, Y_train)\n",
    "scores1 = cross_val_score(dtree, X_train, Y_train, cv=10)\n",
    "print(\"Now the scores after validation are: \",scores1) \n",
    "scores1.mean(), scores1.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a7f08c",
   "metadata": {},
   "source": [
    "# Random Forest "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed9a4a5",
   "metadata": {},
   "source": [
    "Finally we can cross validate for Random forest model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "667f8da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.93      0.95        99\n",
      "           1       0.95      0.93      0.94       101\n",
      "           2       0.92      0.94      0.93       116\n",
      "           3       0.96      0.99      0.98       128\n",
      "           4       0.93      0.92      0.93        90\n",
      "\n",
      "    accuracy                           0.95       534\n",
      "   macro avg       0.95      0.94      0.94       534\n",
      "weighted avg       0.95      0.95      0.95       534\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kedarnandiwdekar/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_split.py:666: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now the scores after validation are:  [0.96       0.968      0.92       0.984      0.984      0.952\n",
      " 0.9516129  0.93548387 0.94354839 0.92741935]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9526064516129032, 0.020825206850704017)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier   \n",
    "rtree = RandomForestClassifier()\n",
    "rtree.fit(X_train, Y_train)\n",
    "Y_pred = rtree.predict(X_test)\n",
    "print(classification_report(Y_test, Y_pred))\n",
    "rtree.fit(X_train, Y_train)\n",
    "scores2 = cross_val_score(rtree, X_train, Y_train, cv=10)\n",
    "print(\"Now the scores after validation are: \",scores2) \n",
    "scores2.mean(), scores2.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba054ab",
   "metadata": {},
   "source": [
    "# Testing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856e9d4b",
   "metadata": {},
   "source": [
    "Now we test our model against test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bbe35882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96        25\n",
      "           1       0.96      0.88      0.92        26\n",
      "           2       0.74      1.00      0.85        26\n",
      "           3       0.96      1.00      0.98        24\n",
      "           4       1.00      0.82      0.90        33\n",
      "\n",
      "    accuracy                           0.92       134\n",
      "   macro avg       0.93      0.92      0.92       134\n",
      "weighted avg       0.93      0.92      0.92       134\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finally we will test our model performance using testing data set. \n",
    "news_test = tfidf.fit_transform(test['text']).toarray()\n",
    "ids_test = test['category_id']\n",
    "news_train, news_test, ids_train, ids_test = train_test_split(news_test,ids_test, test_size = 0.3, random_state = 60,shuffle=True) \n",
    "# test-size denotes what percentage of data will be in the test set. \n",
    "multi.fit(news_train, ids_train)\n",
    "ids_pred = multi.predict(news_test)\n",
    "print(classification_report(ids_test,ids_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23162403",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8791ebe0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
